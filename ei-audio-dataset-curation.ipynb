{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ei-audio-dataset-curation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usmanabbasi123/foundation/blob/main/ei-audio-dataset-curation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMkAgYc8MuCq"
      },
      "source": [
        "# Keyword Spotting Dataset Curation\n",
        "\n",
        "[![Open In Colab <](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ShawnHymel/ei-keyword-spotting/blob/master/ei-audio-dataset-curation.ipynb)\n",
        "\n",
        "Use this tool to download the Google Speech Commands Dataset, combine it with your own keywords, mix in some background noise, and upload the curated dataset to Edge Impulse. From there, you can train a neural network to classify spoken words and upload it to a microcontroller to perform real-time keyword spotting.\n",
        "\n",
        " 1. Upload samples of your own keyword (optional)\n",
        " 2. Adjust parameters in the Settings cell (you will need an [Edge Impulse](https://www.edgeimpulse.com/) account)\n",
        " 3. Run the rest of the cells! ('shift' + 'enter' on each cell)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAn75H1iPYOD"
      },
      "source": [
        "### Upload your own keyword samples\n",
        "You are welcome to use my [custom keyword dataset](https://github.com/ShawnHymel/custom-speech-commands-dataset), but note that it's limited and that I can't promise it will work well. If you want to use it, uncomment the `###Download custom dataset` cell below. You may also add your own recorded keywords to the extracted folder (`/content/custom_keywords`) to augment what's already there.\n",
        "\n",
        "If you'd rather upload your own custom keyword dataset, follow these instructions:\n",
        "\n",
        "On the left pane, in the file browser, create a directory structure containing space for your keyword audio samples. All samples for each keyword should be in a directory with that keyword's name.\n",
        "\n",
        "The audio samples should be `.wav` format, mono, and 1 second long. Bitrate and bitdepth should not matter. Samples shorter than 1 second will be padded with 0s, and samples longer than 1 second will be truncated to 1 second. The exact name of each `.wav` file does not matter, as they will be read, mixed with background noise, and saved to a separate file with an auto-generated name. Directory name does matter (it is used to determine the name of the class during neural network training).\n",
        "\n",
        "Right-click on each keyword directory and upload all of your samples. Your directory structor should look like the following:\n",
        "\n",
        "```\n",
        "/\n",
        "|- content\n",
        "|--- custom_keywords\n",
        "|----- keyword_1\n",
        "|------- 000.wav\n",
        "|------- 001.wav\n",
        "|------- ...\n",
        "|----- keyword_2\n",
        "|------- 000.wav\n",
        "|------- 001.wav\n",
        "|------- ...\n",
        "|----- ...\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81cDNtYQj-ao",
        "outputId": "ffd93cc1-1bbe-4e2d-a7a6-0ac62aecda18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### Update Node.js to the latest stable version\n",
        "!npm cache clean -f\n",
        "!npm install -g n\n",
        "!n 20.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94musing --force\u001b[39m Recommended protections disabled.\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "added 1 package in 830ms\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K  \u001b[36minstalling\u001b[0m : \u001b[2mnode-v20.13.1\u001b[0m\n",
            "  \u001b[36m     mkdir\u001b[0m : \u001b[2m/usr/local/n/versions/node/20.13.1\u001b[0m\n",
            "  \u001b[36m     fetch\u001b[0m : \u001b[2mhttps://nodejs.org/dist/v20.13.1/node-v20.13.1-linux-x64.tar.xz\u001b[0m\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K  \u001b[36m   copying\u001b[0m : \u001b[2mnode/20.13.1\u001b[0m\n",
            "  \u001b[36m installed\u001b[0m : \u001b[2mv20.13.1 (with npm 10.5.2)\u001b[0m\n",
            "\n",
            "Note: the node command changed location and the old location may be remembered in your current shell.\n",
            "  \u001b[36m       old\u001b[0m : \u001b[2m/tools/node/bin/node\u001b[0m\n",
            "  \u001b[36m       new\u001b[0m : \u001b[2m/usr/local/bin/node\u001b[0m\n",
            "If \"node --version\" shows the old version then start a new shell, or reset the location hash with:\n",
            "hash -r  (for bash, zsh, ash, dash, and ksh)\n",
            "rehash   (for csh and tcsh)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMjn7Y0iPXCh",
        "outputId": "5681eb18-c491-44a8-8e52-79cdcbc81737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### Install required packages and tools\n",
        "!python -m pip install soundfile\n",
        "!npm install -g --unsafe-perm edge-impulse-cli@1.26.0 --force"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35musing --force\u001b[0m Recommended protections disabled.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m are-we-there-yet@1.1.7: This package is no longer supported.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m npmlog@4.1.2: This package is no longer supported.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m debug@4.1.1: Debug versions >=3.2.0 <3.2.7 || >=4 <4.3.1 have a low-severity ReDos regression when used in a Node.js environment. It is recommended you upgrade to 3.2.7 or 4.3.1. (https://github.com/visionmedia/debug/issues/797)\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m har-validator@5.1.5: this library is no longer supported\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m osenv@0.1.5: This package is no longer supported.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m copy-concurrently@1.0.5: This package is no longer supported.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m inflight@1.0.6: This module is not supported, and leaks memory. Do not use it. Check out lru-cache if you want a good and tested way to coalesce async requests by a key value, which is much more comprehensive and powerful.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m move-concurrently@1.0.1: This package is no longer supported.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m fs-write-stream-atomic@1.0.10: This package is no longer supported.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m rimraf@2.7.1: Rimraf versions prior to v4 are no longer supported\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m glob@7.2.3: Glob versions prior to v9 are no longer supported\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m uuid@3.4.0: Please upgrade  to version 7 or higher.  Older versions may use Math.random() in certain circumstances, which is known to be problematic.  See https://v8.dev/blog/math-random for details.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m debug@4.1.1: Debug versions >=3.2.0 <3.2.7 || >=4 <4.3.1 have a low-severity ReDos regression when used in a Node.js environment. It is recommended you upgrade to 3.2.7 or 4.3.1. (https://github.com/visionmedia/debug/issues/797)\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m figgy-pudding@3.5.2: This module is no longer supported.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m gauge@2.7.4: This package is no longer supported.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m fstream@1.0.12: This package is no longer supported.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m request-promise@4.2.4: request-promise has been deprecated because it extends the now deprecated request package, see https://github.com/request/request/issues/3142\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m debug@4.1.1: Debug versions >=3.2.0 <3.2.7 || >=4 <4.3.1 have a low-severity ReDos regression when used in a Node.js environment. It is recommended you upgrade to 3.2.7 or 4.3.1. (https://github.com/visionmedia/debug/issues/797)\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m @zeit/dockerignore@0.0.5: \"@zeit/dockerignore\" is no longer maintained\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m @xmldom/xmldom@0.8.8: this version has critical issues, please update to the latest version\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m request@2.88.0: request has been deprecated, see https://github.com/request/request/issues/3142\n",
            "\u001b[K\u001b[?25h\n",
            "added 377 packages in 41s\n",
            "\n",
            "15 packages are looking for funding\n",
            "  run `npm fund` for details\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06YDTU0c0-H"
      },
      "source": [
        "### Settings (You probably do not need to change these)\n",
        "BASE_DIR = \"/content\"\n",
        "OUT_DIR = \"keywords_curated\"\n",
        "GOOGLE_DATASET_FILENAME = \"speech_commands_v0.02.tar.gz\"\n",
        "GOOGLE_DATASET_URL = \"http://download.tensorflow.org/data/\" + GOOGLE_DATASET_FILENAME\n",
        "GOOGLE_DATASET_DIR = \"google_speech_commands\"\n",
        "CUSTOM_KEYWORDS_FILENAME = \"main.zip\"\n",
        "CUSTOM_KEYWORDS_URL = \"https://github.com/ShawnHymel/custom-speech-commands-dataset/archive/\" + CUSTOM_KEYWORDS_FILENAME\n",
        "CUSTOM_KEYWORDS_DIR = \"custom_keywords\"\n",
        "CUSTOM_KEYWORDS_REPO_NAME = \"custom-speech-commands-dataset-main\"\n",
        "CURATION_SCRIPT = \"dataset-curation.py\"\n",
        "CURATION_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/ei-keyword-spotting/master/\" + CURATION_SCRIPT\n",
        "UTILS_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/ei-keyword-spotting/master/utils.py\"\n",
        "NUM_SAMPLES = 1500    # Target number of samples to mix and send to Edge Impulse\n",
        "WORD_VOL = 1.0        # Relative volume of word in output sample\n",
        "BG_VOL = 0.1          # Relative volume of noise in output sample\n",
        "SAMPLE_TIME = 1.0     # Time (seconds) of output sample\n",
        "SAMPLE_RATE = 16000   # Sample rate (Hz) of output sample\n",
        "BIT_DEPTH = \"PCM_16\"  # Options: [PCM_16, PCM_24, PCM_32, PCM_U8, FLOAT, DOUBLE]\n",
        "BG_DIR = \"_background_noise_\"\n",
        "TEST_RATIO = 0.2      # 20% reserved for test set, rest is for training\n",
        "EI_INGEST_TEST_URL = \"https://ingestion.edgeimpulse.com/api/test/data\"\n",
        "EI_INGEST_TRAIN_URL = \"https://ingestion.edgeimpulse.com/api/training/data\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVA3SDiQd-jh",
        "outputId": "936e8e7e-dae1-4614-f31f-5e5464058de7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### Download Google Speech Commands Dataset\n",
        "!cd {BASE_DIR}\n",
        "!wget {GOOGLE_DATASET_URL}\n",
        "!mkdir {GOOGLE_DATASET_DIR}\n",
        "!echo \"Extracting...\"\n",
        "!tar xfz {GOOGLE_DATASET_FILENAME} -C {GOOGLE_DATASET_DIR}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-25 19:55:49--  http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.251.183.207, 209.85.200.207, 64.233.179.207, ...\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.251.183.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2428923189 (2.3G) [application/gzip]\n",
            "Saving to: ‘speech_commands_v0.02.tar.gz’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   2.26G   147MB/s    in 17s     \n",
            "\n",
            "2025-09-25 19:56:05 (138 MB/s) - ‘speech_commands_v0.02.tar.gz’ saved [2428923189/2428923189]\n",
            "\n",
            "Extracting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNGNSwX_d_-E"
      },
      "source": [
        "### Pull out background noise directory\n",
        "!cd {BASE_DIR}\n",
        "!mv \"{GOOGLE_DATASET_DIR}/{BG_DIR}\" \"{BG_DIR}\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1L-vJENeB1S"
      },
      "source": [
        "\n",
        "### (Optional) Download custom dataset--uncomment the code in this cell if you want to use my custom datase\n",
        "\n",
        "## Download, extract, and move dataset to separate directory\n",
        "# !cd {BASE_DIR}\n",
        "# !wget {CUSTOM_KEYWORDS_URL}\n",
        "# !echo \"Extracting...\"\n",
        "# !unzip -q {CUSTOM_KEYWORDS_FILENAME}\n",
        "# !mv \"{CUSTOM_KEYWORDS_REPO_NAME}/{CUSTOM_KEYWORDS_DIR}\" \"{CUSTOM_KEYWORDS_DIR}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nDT4lYEMlY1"
      },
      "source": [
        "### User Settings (do change these)\n",
        "\n",
        "# Location of your custom keyword samples (e.g. \"/content/custom_keywords\")\n",
        "# Leave blank (\"\") for no custom keywords. set to the CUSTOM_KEYWORDS_DIR\n",
        "# variable to use samples from my custom-speech-commands-dataset repo.\n",
        "CUSTOM_DATASET_PATH = \"/content/custom_keywords\"\n",
        "\n",
        "# Edge Impulse > your_project > Dashboard > Keys\n",
        "EI_API_KEY = \"ei_fce106753156002bdf6daa5750260b24dd81bc60fee64394d66c7c1e575170b9\"\n",
        "\n",
        "# Comma separated words. Must match directory names (that contain samples).\n",
        "# Recommended: use 2 keywords for microcontroller demo\n",
        "TARGETS = \"hello, stop\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ve08zgfVLem",
        "outputId": "3abd06cc-3bb9-42ab-b819-b7c7e059dafd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### Download curation and utils scripts\n",
        "!wget {CURATION_SCRIPT_URL}\n",
        "!wget {UTILS_SCRIPT_URL}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-25 20:21:16--  https://raw.githubusercontent.com/ShawnHymel/ei-keyword-spotting/master/dataset-curation.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17427 (17K) [text/plain]\n",
            "Saving to: ‘dataset-curation.py’\n",
            "\n",
            "\rdataset-curation.py   0%[                    ]       0  --.-KB/s               \rdataset-curation.py 100%[===================>]  17.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-25 20:21:16 (53.2 MB/s) - ‘dataset-curation.py’ saved [17427/17427]\n",
            "\n",
            "--2025-09-25 20:21:16--  https://raw.githubusercontent.com/ShawnHymel/ei-keyword-spotting/master/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3966 (3.9K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "utils.py            100%[===================>]   3.87K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-25 20:21:16 (43.1 MB/s) - ‘utils.py’ saved [3966/3966]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O-M_hViZ0ew",
        "outputId": "a26f075f-50bb-45be-8798-29c26cdb81f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### Perform curation and mixing of samples with background noise\n",
        "!cd {BASE_DIR}\n",
        "!python {CURATION_SCRIPT} \\\n",
        "  -t \"{TARGETS}\" \\\n",
        "  -n {NUM_SAMPLES} \\\n",
        "  -w {WORD_VOL} \\\n",
        "  -g {BG_VOL} \\\n",
        "  -s {SAMPLE_TIME} \\\n",
        "  -r {SAMPLE_RATE} \\\n",
        "  -e {BIT_DEPTH} \\\n",
        "  -b \"{BG_DIR}\" \\\n",
        "  -o \"{OUT_DIR}\" \\\n",
        "  \"{GOOGLE_DATASET_DIR}\" \\\n",
        "  \"{CUSTOM_DATASET_PATH}\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------\n",
            "Keyword Dataset Curation Tool\n",
            "v0.1\n",
            "-----------------------------------------------------------------------\n",
            "Gathering random background noise snippets (1500 files)\n",
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
            "Mixing: hello (1500 files)\n",
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
            "Mixing: stop (1500 files)\n",
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
            "Mixing: _unknown (1500 files)\n",
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXd0wH1-hEEX"
      },
      "source": [
        "### Use CLI tool to send curated dataset to Edge Impulse\n",
        "\n",
        "!cd {BASE_DIR}\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Seed with system time\n",
        "random.seed()\n",
        "\n",
        "# Go through each category in our curated dataset\n",
        "for dir in os.listdir(OUT_DIR):\n",
        "\n",
        "  # Create list of files for one category\n",
        "  paths = []\n",
        "  for filename in os.listdir(os.path.join(OUT_DIR, dir)):\n",
        "    paths.append(os.path.join(OUT_DIR, dir, filename))\n",
        "\n",
        "  # Shuffle and divide into test and training sets\n",
        "  random.shuffle(paths)\n",
        "  num_test_samples = int(TEST_RATIO * len(paths))\n",
        "  test_paths = paths[:num_test_samples]\n",
        "  train_paths = paths[num_test_samples:]\n",
        "\n",
        "  # Create arugments list (as a string) for CLI call\n",
        "  test_paths = ['\"' + s + '\"' for s in test_paths]\n",
        "  test_paths = ' '.join(test_paths)\n",
        "  train_paths = ['\"' + s + '\"' for s in train_paths]\n",
        "  train_paths = ' '.join(train_paths)\n",
        "\n",
        "  # Send test files to Edge Impulse\n",
        "  !edge-impulse-uploader \\\n",
        "    --category testing \\\n",
        "    --label {dir} \\\n",
        "    --api-key {EI_API_KEY} \\\n",
        "    --silent \\\n",
        "    {test_paths}\n",
        "\n",
        "  # # Send training files to Edge Impulse\n",
        "  !edge-impulse-uploader \\\n",
        "    --category training \\\n",
        "    --label {dir} \\\n",
        "    --api-key {EI_API_KEY} \\\n",
        "    --silent \\\n",
        "    {train_paths}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}